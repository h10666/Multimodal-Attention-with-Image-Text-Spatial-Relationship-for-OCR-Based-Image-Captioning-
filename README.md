# Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning 

1.1数据集
在[TextCaps数据集](https://textvqa.org/textcaps/)上评估提出的MMA-SR算法性能。该数据集共包含28408幅图像，每幅图像标注有5个描述句子。测试集中的每幅图像除了5句描述外，还有一个额外标注的句子，用于评估人工描述的表现。公平起见，采用与基准方法相同的数据集分割（21953幅图像用于训练，3166幅用于验证，3289幅用于测试）。所有对比方法均使用标准的图像描述评价指标（CIDEr-D、METEOR、BLEU、ROUGE-L和SPICE）及官方发布的相关代码来评价。

1.2代码介绍
1.	数据集为TextCaps，链接为 https://textvqa.org/textcaps/；
2.	run_topdown.sh为算法的训练脚本；
3.	run_eval_test.sh为算法的测试脚本。

1.3论文介绍
读取并描述图像中的场景文字是图像描述生成任务的一大挑战。其生成结果往往存在场景文字不完整或顺序紊乱的问题。为解决该问题，本文着眼于场景文字这一语义概念，提出对其建立空间关系，用这样一种内在的视觉关系加强场景文字之间的关联。首先，从图像中检测出一系列场景文字区域，根据其位置信息计算相对角度，并根据相对角度划分空间关系。然后，在词预测阶段，用空间关系修正候选词的概率，降低了预测场景文字的难度。此外，针对场景文字显著性相对物体较弱导致的信息使用受限问题，提出基于多模态注意力机制的语言模型。该模型可充分利用场景文字和物体中蕴含的信息，加深了对图像的理解，丰富了生成的描述内容。

